# tinycml

**Tiny C Machine Learning Library**

[English](#english) | [TÃ¼rkÃ§e](#tÃ¼rkÃ§e)

---

# English

A tiny, zero-dependency C library for learning machine learning from scratch.

## Overview

tinycml implements fundamental machine learning algorithms in pure C (C11 standard) with zero external dependencies. It's designed for educational purposes, demonstrating how core ML algorithms work under the hood.

## Why tinycml? Advantages Over Modern ML Libraries

### ğŸ¯ Educational Value

| Aspect | tinycml | TensorFlow/PyTorch/scikit-learn |
|--------|---------|--------------------------------|
| **Code Transparency** | Every algorithm is readable, ~100-200 lines each | Thousands of lines, heavy abstractions |
| **Dependencies** | Zero (only standard C library) | Hundreds of packages, complex environments |
| **Understanding** | See exactly how gradient descent works | Black-box functions |
| **Debugging** | Step through with any C debugger | Complex stack traces |

### ğŸš€ Performance Characteristics

| Feature | tinycml | Python ML Libraries |
|---------|---------|---------------------|
| **Startup Time** | Instant (~1ms) | Seconds (import overhead) |
| **Memory Footprint** | ~50KB binary | 100MB+ with dependencies |
| **No GIL** | True parallelism possible | Python GIL limitations |
| **Embedded Systems** | Runs on microcontrollers | Requires full OS |

### ğŸ”§ Use Cases Where This Library Excels

1. **Learning ML Fundamentals**: Understand the math behind algorithms
2. **Embedded/IoT Devices**: Run ML on resource-constrained hardware
3. **Real-time Systems**: Predictable, low-latency inference
4. **Custom Modifications**: Easy to extend and modify algorithms
5. **No-dependency Environments**: Air-gapped systems, minimal containers

### ğŸ“Š When to Use Modern Libraries Instead

- Large-scale training (millions of samples)
- GPU acceleration needed
- Pre-trained models required
- Production systems with established pipelines

## Features

- **Core Linear Algebra**: Matrix operations, vector operations
- **Data Handling**: CSV loading/saving, train/test split, standardization, min-max scaling
- **Supervised Learning**:
  - Linear Regression (closed-form and gradient descent)
  - Logistic Regression (binary classification)
  - k-Nearest Neighbors
- **Unsupervised Learning**:
  - k-Means Clustering
- **Evaluation Metrics**: MSE, RMSE, MAE, Accuracy, Precision, Recall, F1 Score

## Building

### Prerequisites

- C11 compatible compiler (GCC, Clang, MSVC)
- Make (optional, for convenience)
- CMake 3.10+ (alternative build system)

### Quick Start

```bash
# Clone the repository
git clone https://github.com/sametyilmaztemel/tinycml.git
cd tinycml

# Build everything
make

# Run tests
make test

# Run examples
./build/examples/linear_regression_example
./build/examples/logistic_regression_example
./build/examples/knn_example
./build/examples/kmeans_example
```

### Build Options

```bash
make build     # Build library, examples, and tests
make library   # Build only the static library
make examples  # Build example programs
make tests     # Build test programs
make test      # Build and run all tests
make clean     # Remove build artifacts
```

## Detailed Usage Guide

### Step 1: Include Headers

```c
#include "matrix.h"          // Matrix operations
#include "csv.h"             // Data loading
#include "preprocessing.h"   // Data preprocessing
#include "linear_regression.h"
#include "logistic_regression.h"
#include "knn.h"
#include "kmeans.h"
#include "metrics.h"         // Evaluation metrics
```

### Step 2: Load and Prepare Data

```c
// Load CSV file (1 = has header row)
Matrix *data = csv_load("data/mydata.csv", 1);

// Split into features (X) and target (y)
Matrix *X = matrix_alloc(data->rows, data->cols - 1);
Matrix *y = matrix_alloc(data->rows, 1);

for (size_t i = 0; i < data->rows; i++) {
    for (size_t j = 0; j < data->cols - 1; j++) {
        matrix_set(X, i, j, matrix_get(data, i, j));
    }
    matrix_set(y, i, 0, matrix_get(data, i, data->cols - 1));
}

// Add bias column for regression models
Matrix *X_bias = add_bias_column(X);

// Optional: Standardize features
Scaler *scaler = NULL;
Matrix *X_scaled = standardize_fit_transform(X, &scaler);
```

### Step 3: Train/Test Split

```c
// Split data: 80% train, 20% test
TrainTestSplit split = train_test_split(X_bias, y, 0.2, 42);

// Access split data
Matrix *X_train = split.X_train;
Matrix *X_test = split.X_test;
Matrix *y_train = split.y_train;
Matrix *y_test = split.y_test;
```

### Step 4: Train Models

#### Linear Regression

```c
// Method 1: Closed-form solution (fast, exact)
Matrix *weights = linreg_fit_closed(X_train, y_train);

// Method 2: Gradient descent (iterative)
double learning_rate = 0.01;
int epochs = 1000;
Matrix *weights_gd = linreg_fit_gd(X_train, y_train, learning_rate, epochs);
```

#### Logistic Regression

```c
// Binary classification
Matrix *weights = logreg_fit(X_train, y_train, 0.1, 1000);

// Predict probabilities
Matrix *proba = logreg_predict_proba(X_test, weights);

// Predict class labels (threshold = 0.5)
Matrix *predictions = logreg_predict(X_test, weights, 0.5);
```

#### k-Nearest Neighbors

```c
// Fit model (k=5 neighbors)
KNNModel *knn = knn_fit(X_train, y_train, 5);

// Predict
Matrix *predictions = knn_predict(knn, X_test);

// Don't forget to free
knn_free(knn);
```

#### k-Means Clustering

```c
// Cluster into 3 groups
KMeansModel *kmeans = kmeans_fit(X, 3, 100, 42);

// Get cluster assignments
Matrix *labels = kmeans_predict(kmeans, X);

// Access centroids
for (int c = 0; c < kmeans->k; c++) {
    printf("Centroid %d: ", c);
    for (size_t j = 0; j < kmeans->centroids->cols; j++) {
        printf("%.2f ", matrix_get(kmeans->centroids, c, j));
    }
    printf("\n");
}

kmeans_free(kmeans);
```

### Step 5: Evaluate Models

```c
// Regression metrics
double mse_val = mse(y_test, predictions);
double rmse_val = rmse(y_test, predictions);
double mae_val = mae(y_test, predictions);

// Classification metrics
double acc = accuracy(y_test, predictions);
double prec = precision(y_test, predictions);
double rec = recall(y_test, predictions);
double f1 = f1_score(y_test, predictions);

// Confusion matrix
ConfusionMatrix cm = confusion_matrix(y_test, predictions);
confusion_matrix_print(&cm);
```

### Step 6: Memory Management

**IMPORTANT**: Always free allocated memory!

```c
// Free matrices
matrix_free(data);
matrix_free(X);
matrix_free(y);
matrix_free(X_bias);
matrix_free(weights);
matrix_free(predictions);

// Free train/test split
train_test_split_free(&split);

// Free scalers
scaler_free(scaler);
minmax_scaler_free(mm_scaler);

// Free models
knn_free(knn_model);
kmeans_free(kmeans_model);
```

## API Reference

See [docs/API.md](docs/API.md) for complete API documentation.

## Project Structure

```
tinycml/
â”œâ”€â”€ include/           # Public headers
â”‚   â”œâ”€â”€ matrix.h       # Matrix operations
â”‚   â”œâ”€â”€ vector.h       # Vector operations
â”‚   â”œâ”€â”€ utils.h        # Random numbers, statistics
â”‚   â”œâ”€â”€ csv.h          # CSV loading/saving
â”‚   â”œâ”€â”€ preprocessing.h # Data preprocessing
â”‚   â”œâ”€â”€ linear_regression.h
â”‚   â”œâ”€â”€ logistic_regression.h
â”‚   â”œâ”€â”€ knn.h
â”‚   â”œâ”€â”€ kmeans.h
â”‚   â””â”€â”€ metrics.h      # Evaluation metrics
â”œâ”€â”€ src/               # Implementation files
â”œâ”€â”€ examples/          # Runnable CLI demos
â”œâ”€â”€ tests/             # Unit tests
â”œâ”€â”€ data/              # Sample CSV datasets
â”œâ”€â”€ docs/              # Documentation
â”œâ”€â”€ .github/workflows/ # CI configuration
â”œâ”€â”€ CMakeLists.txt     # CMake build
â”œâ”€â”€ Makefile           # Direct build
â””â”€â”€ README.md          # This file
```

## License

MIT License - see LICENSE file for details.

---

# TÃ¼rkÃ§e

Makine Ã¶ÄŸrenmesini sÄ±fÄ±rdan Ã¶ÄŸrenmek iÃ§in Ã¼retim kalitesinde bir C kÃ¼tÃ¼phanesi.

## Genel BakÄ±ÅŸ

Bu kÃ¼tÃ¼phane, temel makine Ã¶ÄŸrenmesi algoritmalarÄ±nÄ± saf C (C11 standardÄ±) ile sÄ±fÄ±r harici baÄŸÄ±mlÄ±lÄ±k kullanarak uygular. EÄŸitim amaÃ§lÄ± tasarlanmÄ±ÅŸ olup, temel ML algoritmalarÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.

## Neden tinycml? Modern ML KÃ¼tÃ¼phanelerine GÃ¶re AvantajlarÄ±

### ğŸ¯ EÄŸitimsel DeÄŸer

| Ã–zellik | tinycml | TensorFlow/PyTorch/scikit-learn |
|---------|-------------------|--------------------------------|
| **Kod ÅeffaflÄ±ÄŸÄ±** | Her algoritma okunabilir, ~100-200 satÄ±r | Binlerce satÄ±r, aÄŸÄ±r soyutlamalar |
| **BaÄŸÄ±mlÄ±lÄ±klar** | SÄ±fÄ±r (sadece standart C kÃ¼tÃ¼phanesi) | YÃ¼zlerce paket, karmaÅŸÄ±k ortamlar |
| **Anlama** | Gradient descent'in tam olarak nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n | Kara kutu fonksiyonlar |
| **Hata AyÄ±klama** | Herhangi bir C debugger ile adÄ±m adÄ±m izleyin | KarmaÅŸÄ±k stack trace'ler |

### ğŸš€ Performans Ã–zellikleri

| Ã–zellik | tinycml | Python ML KÃ¼tÃ¼phaneleri |
|---------|-------------------|------------------------|
| **BaÅŸlangÄ±Ã§ SÃ¼resi** | AnlÄ±k (~1ms) | Saniyeler (import overhead) |
| **Bellek KullanÄ±mÄ±** | ~50KB binary | 100MB+ baÄŸÄ±mlÄ±lÄ±klarla |
| **GIL Yok** | GerÃ§ek paralellik mÃ¼mkÃ¼n | Python GIL sÄ±nÄ±rlamalarÄ± |
| **GÃ¶mÃ¼lÃ¼ Sistemler** | Mikrodenetleyicilerde Ã§alÄ±ÅŸÄ±r | Tam iÅŸletim sistemi gerektirir |

### ğŸ”§ Bu KÃ¼tÃ¼phanenin Ã–ne Ã‡Ä±ktÄ±ÄŸÄ± KullanÄ±m AlanlarÄ±

1. **ML Temellerini Ã–ÄŸrenme**: AlgoritmalarÄ±n arkasÄ±ndaki matematiÄŸi anlayÄ±n
2. **GÃ¶mÃ¼lÃ¼/IoT CihazlarÄ±**: Kaynak kÄ±sÄ±tlÄ± donanÄ±mlarda ML Ã§alÄ±ÅŸtÄ±rÄ±n
3. **GerÃ§ek ZamanlÄ± Sistemler**: Tahmin edilebilir, dÃ¼ÅŸÃ¼k gecikmeli inference
4. **Ã–zel Modifikasyonlar**: AlgoritmalarÄ± geniÅŸletmek ve deÄŸiÅŸtirmek kolay
5. **BaÄŸÄ±mlÄ±lÄ±k Gerektirmeyen Ortamlar**: Ä°zole sistemler, minimal container'lar

### ğŸ“Š Modern KÃ¼tÃ¼phanelerin Tercih Edilmesi Gereken Durumlar

- BÃ¼yÃ¼k Ã¶lÃ§ekli eÄŸitim (milyonlarca Ã¶rnek)
- GPU hÄ±zlandÄ±rma gereksinimi
- Ã–nceden eÄŸitilmiÅŸ modeller gereksinimi
- Kurulu pipeline'lara sahip Ã¼retim sistemleri

## Ã–zellikler

- **Temel Lineer Cebir**: Matris iÅŸlemleri, vektÃ¶r iÅŸlemleri
- **Veri Ä°ÅŸleme**: CSV yÃ¼kleme/kaydetme, train/test bÃ¶lme, standardizasyon, min-max Ã¶lÃ§ekleme
- **Denetimli Ã–ÄŸrenme**:
  - Lineer Regresyon (kapalÄ± form ve gradient descent)
  - Lojistik Regresyon (ikili sÄ±nÄ±flandÄ±rma)
  - k-En YakÄ±n KomÅŸu
- **Denetimsiz Ã–ÄŸrenme**:
  - k-Means KÃ¼meleme
- **DeÄŸerlendirme Metrikleri**: MSE, RMSE, MAE, DoÄŸruluk, Kesinlik, DuyarlÄ±lÄ±k, F1 Skoru

## Derleme

### Gereksinimler

- C11 uyumlu derleyici (GCC, Clang, MSVC)
- Make (isteÄŸe baÄŸlÄ±, kolaylÄ±k iÃ§in)
- CMake 3.10+ (alternatif build sistemi)

### HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# Depoyu klonlayÄ±n
git clone https://github.com/sametyilmaztemel/tinycml.git
cd tinycml

# Her ÅŸeyi derleyin
make

# Testleri Ã§alÄ±ÅŸtÄ±rÄ±n
make test

# Ã–rnekleri Ã§alÄ±ÅŸtÄ±rÄ±n
./build/examples/linear_regression_example
./build/examples/logistic_regression_example
./build/examples/knn_example
./build/examples/kmeans_example
```

## DetaylÄ± KullanÄ±m Rehberi

### AdÄ±m 1: Header DosyalarÄ±nÄ± Dahil Edin

```c
#include "matrix.h"          // Matris iÅŸlemleri
#include "csv.h"             // Veri yÃ¼kleme
#include "preprocessing.h"   // Veri Ã¶n iÅŸleme
#include "linear_regression.h"
#include "logistic_regression.h"
#include "knn.h"
#include "kmeans.h"
#include "metrics.h"         // DeÄŸerlendirme metrikleri
```

### AdÄ±m 2: Veri YÃ¼kleme ve HazÄ±rlama

```c
// CSV dosyasÄ±nÄ± yÃ¼kle (1 = baÅŸlÄ±k satÄ±rÄ± var)
Matrix *data = csv_load("data/mydata.csv", 1);

// Ã–zellikler (X) ve hedef (y) olarak ayÄ±r
Matrix *X = matrix_alloc(data->rows, data->cols - 1);
Matrix *y = matrix_alloc(data->rows, 1);

for (size_t i = 0; i < data->rows; i++) {
    for (size_t j = 0; j < data->cols - 1; j++) {
        matrix_set(X, i, j, matrix_get(data, i, j));
    }
    matrix_set(y, i, 0, matrix_get(data, i, data->cols - 1));
}

// Regresyon modelleri iÃ§in bias sÃ¼tunu ekle
Matrix *X_bias = add_bias_column(X);

// Ä°steÄŸe baÄŸlÄ±: Ã–zellikleri standardize et
Scaler *scaler = NULL;
Matrix *X_scaled = standardize_fit_transform(X, &scaler);
```

### AdÄ±m 3: Train/Test BÃ¶lmesi

```c
// Veriyi bÃ¶l: %80 eÄŸitim, %20 test
TrainTestSplit split = train_test_split(X_bias, y, 0.2, 42);

// BÃ¶lÃ¼nmÃ¼ÅŸ verilere eriÅŸ
Matrix *X_train = split.X_train;
Matrix *X_test = split.X_test;
Matrix *y_train = split.y_train;
Matrix *y_test = split.y_test;
```

### AdÄ±m 4: Modelleri EÄŸitin

#### Lineer Regresyon

```c
// YÃ¶ntem 1: KapalÄ± form Ã§Ã¶zÃ¼mÃ¼ (hÄ±zlÄ±, kesin)
Matrix *weights = linreg_fit_closed(X_train, y_train);

// YÃ¶ntem 2: Gradient descent (iteratif)
double learning_rate = 0.01;
int epochs = 1000;
Matrix *weights_gd = linreg_fit_gd(X_train, y_train, learning_rate, epochs);
```

#### Lojistik Regresyon

```c
// Ä°kili sÄ±nÄ±flandÄ±rma
Matrix *weights = logreg_fit(X_train, y_train, 0.1, 1000);

// OlasÄ±lÄ±klarÄ± tahmin et
Matrix *proba = logreg_predict_proba(X_test, weights);

// SÄ±nÄ±f etiketlerini tahmin et (eÅŸik = 0.5)
Matrix *predictions = logreg_predict(X_test, weights, 0.5);
```

#### k-En YakÄ±n KomÅŸu

```c
// Modeli fit et (k=5 komÅŸu)
KNNModel *knn = knn_fit(X_train, y_train, 5);

// Tahmin et
Matrix *predictions = knn_predict(knn, X_test);

// BelleÄŸi temizlemeyi unutmayÄ±n
knn_free(knn);
```

#### k-Means KÃ¼meleme

```c
// 3 gruba kÃ¼melendir
KMeansModel *kmeans = kmeans_fit(X, 3, 100, 42);

// KÃ¼me atamalarÄ±nÄ± al
Matrix *labels = kmeans_predict(kmeans, X);

// Merkez noktalarÄ±na eriÅŸ
for (int c = 0; c < kmeans->k; c++) {
    printf("Merkez %d: ", c);
    for (size_t j = 0; j < kmeans->centroids->cols; j++) {
        printf("%.2f ", matrix_get(kmeans->centroids, c, j));
    }
    printf("\n");
}

kmeans_free(kmeans);
```

### AdÄ±m 5: Modelleri DeÄŸerlendirin

```c
// Regresyon metrikleri
double mse_val = mse(y_test, predictions);
double rmse_val = rmse(y_test, predictions);
double mae_val = mae(y_test, predictions);

// SÄ±nÄ±flandÄ±rma metrikleri
double acc = accuracy(y_test, predictions);
double prec = precision(y_test, predictions);
double rec = recall(y_test, predictions);
double f1 = f1_score(y_test, predictions);

// KarÄ±ÅŸÄ±klÄ±k matrisi
ConfusionMatrix cm = confusion_matrix(y_test, predictions);
confusion_matrix_print(&cm);
```

### AdÄ±m 6: Bellek YÃ¶netimi

**Ã–NEMLÄ°**: AyrÄ±lan belleÄŸi her zaman serbest bÄ±rakÄ±n!

```c
// Matrisleri serbest bÄ±rak
matrix_free(data);
matrix_free(X);
matrix_free(y);
matrix_free(X_bias);
matrix_free(weights);
matrix_free(predictions);

// Train/test bÃ¶lmesini serbest bÄ±rak
train_test_split_free(&split);

// Ã–lÃ§ekleyicileri serbest bÄ±rak
scaler_free(scaler);
minmax_scaler_free(mm_scaler);

// Modelleri serbest bÄ±rak
knn_free(knn_model);
kmeans_free(kmeans_model);
```

## API ReferansÄ±

Tam API dokÃ¼mantasyonu iÃ§in [docs/API_TR.md](docs/API_TR.md) dosyasÄ±na bakÄ±n.

## Proje YapÄ±sÄ±

```
tinycml/
â”œâ”€â”€ include/           # Genel baÅŸlÄ±k dosyalarÄ±
â”‚   â”œâ”€â”€ matrix.h       # Matris iÅŸlemleri
â”‚   â”œâ”€â”€ vector.h       # VektÃ¶r iÅŸlemleri
â”‚   â”œâ”€â”€ utils.h        # Rastgele sayÄ±lar, istatistikler
â”‚   â”œâ”€â”€ csv.h          # CSV yÃ¼kleme/kaydetme
â”‚   â”œâ”€â”€ preprocessing.h # Veri Ã¶n iÅŸleme
â”‚   â”œâ”€â”€ linear_regression.h
â”‚   â”œâ”€â”€ logistic_regression.h
â”‚   â”œâ”€â”€ knn.h
â”‚   â”œâ”€â”€ kmeans.h
â”‚   â””â”€â”€ metrics.h      # DeÄŸerlendirme metrikleri
â”œâ”€â”€ src/               # Uygulama dosyalarÄ±
â”œâ”€â”€ examples/          # Ã‡alÄ±ÅŸtÄ±rÄ±labilir CLI demolarÄ±
â”œâ”€â”€ tests/             # Birim testleri
â”œâ”€â”€ data/              # Ã–rnek CSV veri setleri
â”œâ”€â”€ docs/              # DokÃ¼mantasyon
â”œâ”€â”€ .github/workflows/ # CI yapÄ±landÄ±rmasÄ±
â”œâ”€â”€ CMakeLists.txt     # CMake build
â”œâ”€â”€ Makefile           # DoÄŸrudan build
â””â”€â”€ README.md          # Bu dosya
```

## Lisans

MIT LisansÄ± - detaylar iÃ§in LICENSE dosyasÄ±na bakÄ±n.
