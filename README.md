<p align="center">
  <img src="assets/logo.png" alt="tinycml logo" width="300">
</p>

<h1 align="center">tinycml</h1>

<p align="center">
  <strong>Tiny C Machine Learning Library</strong>
</p>

<p align="center">
  <a href="#english">English</a> | <a href="#tÃ¼rkÃ§e">TÃ¼rkÃ§e</a>
</p>

---

# English

A comprehensive, zero-dependency C library implementing scikit-learn style machine learning in pure C.

## Overview

tinycml implements a wide range of machine learning algorithms in pure C (C11 standard) with zero external dependencies. It provides a unified scikit-learn style API (`fit`/`predict`/`score`) while maintaining C's advantages: instant startup, tiny binary size, and embedded system compatibility.

## Library Statistics

| Metric | Value |
|--------|-------|
| **Library Size** | ~160KB |
| **Lines of Code** | ~9,700 |
| **Dependencies** | Zero |
| **Startup Time** | ~1ms |
| **C Standard** | C11 |

## Why tinycml? Advantages Over Modern ML Libraries

### ğŸ¯ Educational Value

| Aspect | tinycml | TensorFlow/PyTorch/scikit-learn |
|--------|---------|--------------------------------|
| **Code Transparency** | Every algorithm is readable, ~100-300 lines each | Thousands of lines, heavy abstractions |
| **Dependencies** | Zero (only standard C library) | Hundreds of packages, complex environments |
| **Understanding** | See exactly how gradient descent, backprop work | Black-box functions |
| **Debugging** | Step through with any C debugger | Complex stack traces |

### ğŸš€ Performance Characteristics

| Feature | tinycml | Python ML Libraries |
|---------|---------|---------------------|
| **Startup Time** | Instant (~1ms) | Seconds (import overhead) |
| **Memory Footprint** | ~160KB binary | 100MB+ with dependencies |
| **No GIL** | True parallelism possible | Python GIL limitations |
| **Embedded Systems** | Runs on microcontrollers | Requires full OS |

### ğŸ”§ Use Cases Where This Library Excels

1. **Learning ML Fundamentals**: Understand the math behind algorithms
2. **Embedded/IoT Devices**: Run ML on resource-constrained hardware
3. **Real-time Systems**: Predictable, low-latency inference
4. **Custom Modifications**: Easy to extend and modify algorithms
5. **No-dependency Environments**: Air-gapped systems, minimal containers

## Features

### Core Infrastructure
- **Unified Estimator API**: scikit-learn style `fit`/`predict`/`score` interface
- **Pipeline System**: Chain preprocessing steps with models
- **Cross-Validation**: K-Fold, Stratified K-Fold with scoring
- **Model Selection**: GridSearchCV for hyperparameter tuning
- **Model Serialization**: Save/load trained models to binary files

### Supervised Learning
- **Linear Regression** (closed-form and gradient descent)
- **Logistic Regression** (binary classification with L2 regularization)
- **k-Nearest Neighbors** (classification and regression)
- **Naive Bayes** (Gaussian)
- **Decision Tree** (classification with Gini/Entropy criteria)
- **Random Forest** (ensemble with bootstrap, OOB score)
- **Neural Network** (feedforward with backpropagation, multiple activations)
- **Support Vector Machine** (linear SVM)

### Unsupervised Learning
- **k-Means Clustering** (with k-means++ initialization)
- **PCA** (Principal Component Analysis with whitening)

### Feature Engineering
- **Feature Selection**: SelectKBest, VarianceThreshold
- **Scoring Functions**: f_classif, f_regression, chi2, mutual_info
- **Preprocessing**: StandardScaler, MinMaxScaler, OneHotEncoder, PolynomialFeatures

### Evaluation
- **Regression Metrics**: MSE, RMSE, MAE, RÂ²
- **Classification Metrics**: Accuracy, Precision, Recall, F1, Confusion Matrix
- **Clustering Metrics**: Inertia, Silhouette Score

## Building

### Prerequisites

- C11 compatible compiler (GCC, Clang, MSVC)
- Make (optional, for convenience)

### Quick Start

```bash
# Clone the repository
git clone https://github.com/sametyilmaztemel/tinycml.git
cd tinycml

# Build everything
make

# Run tests
make test

# Run examples
./build/examples/linear_regression_example
./build/examples/random_forest_example
./build/examples/neural_network_example
./build/examples/pca_example
./build/examples/feature_selection_example
```

### Build Options

```bash
make build     # Build library, examples, and tests
make library   # Build only the static library
make examples  # Build example programs
make tests     # Build test programs
make test      # Build and run all tests
make clean     # Remove build artifacts
```

## Usage Guide

### The Unified Estimator API

All models in tinycml follow a consistent interface inspired by scikit-learn:

```c
#include "estimator.h"
#include "linear_regression.h"

// Create model
LinearRegression *model = linear_regression_create(LINREG_SOLVER_CLOSED);

// Train
model->base.fit((Estimator*)model, X_train, y_train);

// Predict
Matrix *predictions = model->base.predict((Estimator*)model, X_test);

// Evaluate
double r2 = model->base.score((Estimator*)model, X_test, y_test);

// Free
model->base.free((Estimator*)model);
```

### Pipeline: Chain Preprocessing with Models

```c
#include "pipeline.h"
#include "preprocessing.h"
#include "linear_regression.h"

// Create pipeline with preprocessing + model
Pipeline *pipe = pipeline_create();
pipeline_add_step(pipe, "scaler", (Estimator*)standard_scaler_create());
pipeline_add_step(pipe, "model", (Estimator*)linear_regression_create(LINREG_SOLVER_CLOSED));

// Fit entire pipeline
pipe->base.fit((Estimator*)pipe, X_train, y_train);

// Predict (automatically applies all transformations)
Matrix *pred = pipe->base.predict((Estimator*)pipe, X_test);

// Score
double score = pipe->base.score((Estimator*)pipe, X_test, y_test);

pipeline_free(pipe);
```

### Cross-Validation

```c
#include "validation.h"
#include "logistic_regression.h"

LogisticRegression *model = logistic_regression_create_full(0.01, 1000, 0.0);

// 5-fold cross-validation
CrossValResults *cv = cross_val_score((Estimator*)model, X, y, 5, 1, 42);

printf("Mean accuracy: %.4f (+/- %.4f)\n", cv->mean_test_score, cv->std_test_score);

cross_val_results_free(cv);
model->base.free((Estimator*)model);
```

### Hyperparameter Tuning with GridSearchCV

```c
#include "model_selection.h"
#include "decision_tree.h"

// Define parameter grid
ParamGrid grid;
param_grid_init(&grid);
param_grid_add_int(&grid, "max_depth", (int[]){3, 5, 10}, 3);
param_grid_add_int(&grid, "min_samples_split", (int[]){2, 5, 10}, 3);

// Create GridSearchCV
DecisionTreeClassifier *dt = decision_tree_classifier_create();
GridSearchCV *gs = grid_search_cv_create((Estimator*)dt, &grid, 5, 42);

// Fit (searches all parameter combinations)
gs->base.fit((Estimator*)gs, X, y);

printf("Best score: %.4f\n", gs->best_score_);
printf("Best max_depth: %d\n", grid_search_get_best_int(gs, "max_depth"));

grid_search_cv_free(gs);
param_grid_free(&grid);
```

### Random Forest

```c
#include "ensemble.h"

// Create Random Forest with 100 trees
RandomForestClassifier *rf = random_forest_classifier_create_full(
    100,    // n_estimators
    10,     // max_depth
    2,      // min_samples_split
    1,      // min_samples_leaf
    0,      // max_features (0 = sqrt)
    1,      // bootstrap
    42      // random_state
);

rf->base.fit((Estimator*)rf, X_train, y_train);

double accuracy = rf->base.score((Estimator*)rf, X_test, y_test);
printf("Test accuracy: %.4f\n", accuracy);
printf("OOB score: %.4f\n", rf->oob_score_);

// Probability predictions
Matrix *proba = rf->base.predict_proba((Estimator*)rf, X_test);

rf->base.free((Estimator*)rf);
```

### Neural Network

```c
#include "neural_network.h"

// Create network: input -> 64 -> 32 -> output
size_t layer_sizes[] = {n_features, 64, 32, n_classes};
NeuralNetwork *nn = neural_network_create(layer_sizes, 4, ACTIVATION_RELU);

// Configure training
nn->learning_rate = 0.001;
nn->epochs = 100;
nn->batch_size = 32;

nn->base.fit((Estimator*)nn, X_train, y_train);

double accuracy = nn->base.score((Estimator*)nn, X_test, y_test);
printf("Neural network accuracy: %.4f\n", accuracy);

nn->base.free((Estimator*)nn);
```

### PCA (Dimensionality Reduction)

```c
#include "decomposition.h"

// Reduce to 2 principal components
PCA *pca = pca_create(2);
pca->base.fit((Estimator*)pca, X, NULL);

// Transform data
Matrix *X_reduced = pca->base.transform((Estimator*)pca, X);

// Check explained variance
const double *evr = pca_explained_variance_ratio(pca);
printf("PC1 explains %.2f%% of variance\n", evr[0] * 100);
printf("PC2 explains %.2f%% of variance\n", evr[1] * 100);

// Reconstruct original data
Matrix *X_reconstructed = pca_inverse_transform(pca, X_reduced);

pca->base.free((Estimator*)pca);
```

### Feature Selection

```c
#include "feature_selection.h"

// SelectKBest: Keep top 5 features by F-score
SelectKBest *selector = select_k_best_create(SCORE_F_REGRESSION, 5);
selector->base.fit((Estimator*)selector, X, y);

// Get selected feature indices
const int *support = select_k_best_get_support(selector);

// Transform data to selected features only
Matrix *X_selected = selector->base.transform((Estimator*)selector, X);

// VarianceThreshold: Remove low-variance features
VarianceThreshold *vt = variance_threshold_create(0.1);
vt->base.fit((Estimator*)vt, X, NULL);
Matrix *X_filtered = vt->base.transform((Estimator*)vt, X);

selector->base.free((Estimator*)selector);
vt->base.free((Estimator*)vt);
```

### Model Serialization

```c
// Save trained model
model->base.save((Estimator*)model, "model.bin");

// Load model
LinearRegression *loaded = (LinearRegression*)linear_regression_load("model.bin");
```

### Training Progress and Callbacks

```c
#include "estimator.h"

// Enable verbose output
model->base.verbose = VERBOSE_PROGRESS;

// Or use custom callback
void my_callback(int epoch, double loss, double metric, void *data) {
    printf("Epoch %d: loss=%.4f, metric=%.4f\n", epoch, loss, metric);
}

estimator_set_callback((Estimator*)model, my_callback, NULL);

// After training, access history
const TrainingHistory *history = estimator_get_history((Estimator*)model);
```

## Examples

The library includes comprehensive examples:

| Example | Description |
|---------|-------------|
| `linear_regression_example` | Closed-form vs gradient descent |
| `logistic_regression_example` | Binary classification |
| `knn_example` | k-Nearest Neighbors |
| `kmeans_example` | Clustering with k-means++ |
| `estimator_api_example` | Unified API demonstration |
| `cross_validation_example` | K-Fold cross-validation |
| `pipeline_example` | Preprocessing + model chains |
| `random_forest_example` | Ensemble learning |
| `pca_example` | Dimensionality reduction |
| `feature_selection_example` | Feature importance and selection |

## Project Structure

```
tinycml/
â”œâ”€â”€ include/              # Public headers
â”‚   â”œâ”€â”€ matrix.h          # Matrix operations
â”‚   â”œâ”€â”€ estimator.h       # Unified estimator API
â”‚   â”œâ”€â”€ pipeline.h        # Pipeline system
â”‚   â”œâ”€â”€ validation.h      # Cross-validation
â”‚   â”œâ”€â”€ model_selection.h # GridSearchCV
â”‚   â”œâ”€â”€ linear_regression.h
â”‚   â”œâ”€â”€ logistic_regression.h
â”‚   â”œâ”€â”€ knn.h
â”‚   â”œâ”€â”€ kmeans.h
â”‚   â”œâ”€â”€ naive_bayes.h
â”‚   â”œâ”€â”€ decision_tree.h
â”‚   â”œâ”€â”€ ensemble.h        # Random Forest
â”‚   â”œâ”€â”€ neural_network.h
â”‚   â”œâ”€â”€ decomposition.h   # PCA
â”‚   â”œâ”€â”€ feature_selection.h
â”‚   â”œâ”€â”€ preprocessing.h
â”‚   â””â”€â”€ metrics.h
â”œâ”€â”€ src/                  # Implementation files
â”œâ”€â”€ examples/             # Runnable demos
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ data/                 # Sample datasets
â””â”€â”€ docs/                 # Documentation
```

## License

MIT License - see LICENSE file for details.

---

# TÃ¼rkÃ§e

Saf C ile scikit-learn tarzÄ± makine Ã¶ÄŸrenmesi uygulayan kapsamlÄ±, sÄ±fÄ±r baÄŸÄ±mlÄ±lÄ±klÄ± bir C kÃ¼tÃ¼phanesi.

## Genel BakÄ±ÅŸ

tinycml, geniÅŸ bir makine Ã¶ÄŸrenmesi algoritmasÄ± yelpazesini saf C (C11 standardÄ±) ile sÄ±fÄ±r harici baÄŸÄ±mlÄ±lÄ±k kullanarak uygular. BirleÅŸik scikit-learn tarzÄ± API (`fit`/`predict`/`score`) sunarken C'nin avantajlarÄ±nÄ± korur: anlÄ±k baÅŸlangÄ±Ã§, kÃ¼Ã§Ã¼k binary boyutu ve gÃ¶mÃ¼lÃ¼ sistem uyumluluÄŸu.

## KÃ¼tÃ¼phane Ä°statistikleri

| Metrik | DeÄŸer |
|--------|-------|
| **KÃ¼tÃ¼phane Boyutu** | ~160KB |
| **Kod SatÄ±rÄ±** | ~9,700 |
| **BaÄŸÄ±mlÄ±lÄ±k** | SÄ±fÄ±r |
| **BaÅŸlangÄ±Ã§ SÃ¼resi** | ~1ms |
| **C StandardÄ±** | C11 |

## Ã–zellikler

### Temel AltyapÄ±
- **BirleÅŸik Estimator API'si**: scikit-learn tarzÄ± `fit`/`predict`/`score` arayÃ¼zÃ¼
- **Pipeline Sistemi**: Ã–n iÅŸleme adÄ±mlarÄ±nÄ± modellerle zincirleyin
- **Ã‡apraz DoÄŸrulama**: K-Fold, Stratified K-Fold
- **Model SeÃ§imi**: Hiperparametre ayarÄ± iÃ§in GridSearchCV
- **Model SerileÅŸtirme**: EÄŸitilmiÅŸ modelleri kaydet/yÃ¼kle

### Denetimli Ã–ÄŸrenme
- **Lineer Regresyon** (kapalÄ± form ve gradient descent)
- **Lojistik Regresyon** (L2 dÃ¼zenlileÅŸtirmeli ikili sÄ±nÄ±flandÄ±rma)
- **k-En YakÄ±n KomÅŸu** (sÄ±nÄ±flandÄ±rma ve regresyon)
- **Naive Bayes** (Gaussian)
- **Karar AÄŸacÄ±** (Gini/Entropi kriterleriyle sÄ±nÄ±flandÄ±rma)
- **Rastgele Orman** (bootstrap ile topluluk, OOB skoru)
- **Sinir AÄŸÄ±** (geri yayÄ±lÄ±m ile ileri beslemeli, Ã§oklu aktivasyonlar)
- **Destek VektÃ¶r Makinesi** (lineer SVM)

### Denetimsiz Ã–ÄŸrenme
- **k-Means KÃ¼meleme** (k-means++ baÅŸlatma ile)
- **PCA** (Beyazlatma ile Temel BileÅŸen Analizi)

### Ã–zellik MÃ¼hendisliÄŸi
- **Ã–zellik SeÃ§imi**: SelectKBest, VarianceThreshold
- **Puanlama FonksiyonlarÄ±**: f_classif, f_regression, chi2, mutual_info
- **Ã–n Ä°ÅŸleme**: StandardScaler, MinMaxScaler, OneHotEncoder, PolynomialFeatures

### DeÄŸerlendirme
- **Regresyon Metrikleri**: MSE, RMSE, MAE, RÂ²
- **SÄ±nÄ±flandÄ±rma Metrikleri**: DoÄŸruluk, Kesinlik, DuyarlÄ±lÄ±k, F1, KarÄ±ÅŸÄ±klÄ±k Matrisi
- **KÃ¼meleme Metrikleri**: Atalet, Silhouette Skoru

## Derleme

### Gereksinimler

- C11 uyumlu derleyici (GCC, Clang, MSVC)
- Make (isteÄŸe baÄŸlÄ±)

### HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# Depoyu klonlayÄ±n
git clone https://github.com/sametyilmaztemel/tinycml.git
cd tinycml

# Her ÅŸeyi derleyin
make

# Testleri Ã§alÄ±ÅŸtÄ±rÄ±n
make test

# Ã–rnekleri Ã§alÄ±ÅŸtÄ±rÄ±n
./build/examples/linear_regression_example
./build/examples/random_forest_example
./build/examples/neural_network_example
./build/examples/pca_example
./build/examples/feature_selection_example
```

## KullanÄ±m Rehberi

### BirleÅŸik Estimator API'si

tinycml'deki tÃ¼m modeller scikit-learn'den esinlenen tutarlÄ± bir arayÃ¼z izler:

```c
#include "estimator.h"
#include "linear_regression.h"

// Model oluÅŸtur
LinearRegression *model = linear_regression_create(LINREG_SOLVER_CLOSED);

// EÄŸit
model->base.fit((Estimator*)model, X_train, y_train);

// Tahmin et
Matrix *predictions = model->base.predict((Estimator*)model, X_test);

// DeÄŸerlendir
double r2 = model->base.score((Estimator*)model, X_test, y_test);

// Serbest bÄ±rak
model->base.free((Estimator*)model);
```

### Pipeline: Ã–n Ä°ÅŸlemeyi Modellerle Zincirleyin

```c
#include "pipeline.h"
#include "preprocessing.h"
#include "linear_regression.h"

// Ã–n iÅŸleme + model ile pipeline oluÅŸtur
Pipeline *pipe = pipeline_create();
pipeline_add_step(pipe, "scaler", (Estimator*)standard_scaler_create());
pipeline_add_step(pipe, "model", (Estimator*)linear_regression_create(LINREG_SOLVER_CLOSED));

// TÃ¼m pipeline'Ä± eÄŸit
pipe->base.fit((Estimator*)pipe, X_train, y_train);

// Tahmin et (tÃ¼m dÃ¶nÃ¼ÅŸÃ¼mleri otomatik uygular)
Matrix *pred = pipe->base.predict((Estimator*)pipe, X_test);

pipeline_free(pipe);
```

### Ã‡apraz DoÄŸrulama

```c
#include "validation.h"
#include "logistic_regression.h"

LogisticRegression *model = logistic_regression_create_full(0.01, 1000, 0.0);

// 5-katlÄ± Ã§apraz doÄŸrulama
CrossValResults *cv = cross_val_score((Estimator*)model, X, y, 5, 1, 42);

printf("Ortalama doÄŸruluk: %.4f (+/- %.4f)\n", cv->mean_test_score, cv->std_test_score);

cross_val_results_free(cv);
model->base.free((Estimator*)model);
```

### GridSearchCV ile Hiperparametre AyarÄ±

```c
#include "model_selection.h"
#include "decision_tree.h"

// Parametre Ä±zgarasÄ± tanÄ±mla
ParamGrid grid;
param_grid_init(&grid);
param_grid_add_int(&grid, "max_depth", (int[]){3, 5, 10}, 3);
param_grid_add_int(&grid, "min_samples_split", (int[]){2, 5, 10}, 3);

// GridSearchCV oluÅŸtur
DecisionTreeClassifier *dt = decision_tree_classifier_create();
GridSearchCV *gs = grid_search_cv_create((Estimator*)dt, &grid, 5, 42);

// EÄŸit (tÃ¼m parametre kombinasyonlarÄ±nÄ± arar)
gs->base.fit((Estimator*)gs, X, y);

printf("En iyi skor: %.4f\n", gs->best_score_);

grid_search_cv_free(gs);
param_grid_free(&grid);
```

### Rastgele Orman

```c
#include "ensemble.h"

// 100 aÄŸaÃ§lÄ± Rastgele Orman oluÅŸtur
RandomForestClassifier *rf = random_forest_classifier_create_full(
    100,    // n_estimators
    10,     // max_depth
    2,      // min_samples_split
    1,      // min_samples_leaf
    0,      // max_features (0 = sqrt)
    1,      // bootstrap
    42      // random_state
);

rf->base.fit((Estimator*)rf, X_train, y_train);

double accuracy = rf->base.score((Estimator*)rf, X_test, y_test);
printf("Test doÄŸruluÄŸu: %.4f\n", accuracy);
printf("OOB skoru: %.4f\n", rf->oob_score_);

rf->base.free((Estimator*)rf);
```

### Sinir AÄŸÄ±

```c
#include "neural_network.h"

// AÄŸ oluÅŸtur: girdi -> 64 -> 32 -> Ã§Ä±ktÄ±
size_t layer_sizes[] = {n_features, 64, 32, n_classes};
NeuralNetwork *nn = neural_network_create(layer_sizes, 4, ACTIVATION_RELU);

// EÄŸitimi yapÄ±landÄ±r
nn->learning_rate = 0.001;
nn->epochs = 100;
nn->batch_size = 32;

nn->base.fit((Estimator*)nn, X_train, y_train);

double accuracy = nn->base.score((Estimator*)nn, X_test, y_test);
printf("Sinir aÄŸÄ± doÄŸruluÄŸu: %.4f\n", accuracy);

nn->base.free((Estimator*)nn);
```

### PCA (Boyut Ä°ndirgeme)

```c
#include "decomposition.h"

// 2 temel bileÅŸene indirge
PCA *pca = pca_create(2);
pca->base.fit((Estimator*)pca, X, NULL);

// Veriyi dÃ¶nÃ¼ÅŸtÃ¼r
Matrix *X_reduced = pca->base.transform((Estimator*)pca, X);

// AÃ§Ä±klanan varyansÄ± kontrol et
const double *evr = pca_explained_variance_ratio(pca);
printf("PC1 varyansÄ±n %%%.2f'sini aÃ§Ä±klar\n", evr[0] * 100);

// Orijinal veriyi yeniden oluÅŸtur
Matrix *X_reconstructed = pca_inverse_transform(pca, X_reduced);

pca->base.free((Estimator*)pca);
```

### Ã–zellik SeÃ§imi

```c
#include "feature_selection.h"

// SelectKBest: F-skoruna gÃ¶re en iyi 5 Ã¶zelliÄŸi tut
SelectKBest *selector = select_k_best_create(SCORE_F_REGRESSION, 5);
selector->base.fit((Estimator*)selector, X, y);

// SeÃ§ilen Ã¶zellik indekslerini al
const int *support = select_k_best_get_support(selector);

// Veriyi sadece seÃ§ilen Ã¶zelliklere dÃ¶nÃ¼ÅŸtÃ¼r
Matrix *X_selected = selector->base.transform((Estimator*)selector, X);

// VarianceThreshold: DÃ¼ÅŸÃ¼k varyanslÄ± Ã¶zellikleri kaldÄ±r
VarianceThreshold *vt = variance_threshold_create(0.1);
vt->base.fit((Estimator*)vt, X, NULL);
Matrix *X_filtered = vt->base.transform((Estimator*)vt, X);

selector->base.free((Estimator*)selector);
vt->base.free((Estimator*)vt);
```

## Ã–rnekler

KÃ¼tÃ¼phane kapsamlÄ± Ã¶rnekler iÃ§erir:

| Ã–rnek | AÃ§Ä±klama |
|-------|----------|
| `linear_regression_example` | KapalÄ± form vs gradient descent |
| `logistic_regression_example` | Ä°kili sÄ±nÄ±flandÄ±rma |
| `knn_example` | k-En YakÄ±n KomÅŸu |
| `kmeans_example` | k-means++ ile kÃ¼meleme |
| `estimator_api_example` | BirleÅŸik API gÃ¶sterimi |
| `cross_validation_example` | K-Fold Ã§apraz doÄŸrulama |
| `pipeline_example` | Ã–n iÅŸleme + model zincirleri |
| `random_forest_example` | Topluluk Ã¶ÄŸrenmesi |
| `pca_example` | Boyut indirgeme |
| `feature_selection_example` | Ã–zellik Ã¶nemi ve seÃ§imi |

## Proje YapÄ±sÄ±

```
tinycml/
â”œâ”€â”€ include/              # Genel baÅŸlÄ±k dosyalarÄ±
â”‚   â”œâ”€â”€ matrix.h          # Matris iÅŸlemleri
â”‚   â”œâ”€â”€ estimator.h       # BirleÅŸik estimator API'si
â”‚   â”œâ”€â”€ pipeline.h        # Pipeline sistemi
â”‚   â”œâ”€â”€ validation.h      # Ã‡apraz doÄŸrulama
â”‚   â”œâ”€â”€ model_selection.h # GridSearchCV
â”‚   â”œâ”€â”€ linear_regression.h
â”‚   â”œâ”€â”€ logistic_regression.h
â”‚   â”œâ”€â”€ knn.h
â”‚   â”œâ”€â”€ kmeans.h
â”‚   â”œâ”€â”€ naive_bayes.h
â”‚   â”œâ”€â”€ decision_tree.h
â”‚   â”œâ”€â”€ ensemble.h        # Rastgele Orman
â”‚   â”œâ”€â”€ neural_network.h
â”‚   â”œâ”€â”€ decomposition.h   # PCA
â”‚   â”œâ”€â”€ feature_selection.h
â”‚   â”œâ”€â”€ preprocessing.h
â”‚   â””â”€â”€ metrics.h
â”œâ”€â”€ src/                  # Uygulama dosyalarÄ±
â”œâ”€â”€ examples/             # Ã‡alÄ±ÅŸtÄ±rÄ±labilir demolar
â”œâ”€â”€ tests/                # Birim testleri
â”œâ”€â”€ data/                 # Ã–rnek veri setleri
â””â”€â”€ docs/                 # DokÃ¼mantasyon
```

## Lisans

MIT LisansÄ± - detaylar iÃ§in LICENSE dosyasÄ±na bakÄ±n.
